# -*- coding: utf-8 -*-
"""lambdadata.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1q15N967tG9U0cl_TFiASCG_nkNeh-VH1
"""

#general quick functions for datascience

import pprint

#DF CLEANING AND INFO

def nullcheck(df):
  '''checks for nulls in a dataframe'''
  pp = pprint.PrettyPrinter(indent=4)
  pp.pprint(df.isna().sum())
  
def whitespace(df):
  '''removes whitespace from dataframe'''
  df = df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
  return df

def remove_percent(df):
  df = df.str.strip('%')
  df = df.astype(float)
  return x

#statistics
 
def find_outliers(list):
  elements = np.array(arr)
  mean = np.mean(elements, axis=0)
  sd = np.std(elements, axis=0)
  final_list = [x for x in arr if (x > mean - 2 * sd)]
  final_list = [x for x in final_list if (x < mean + 2 * sd)]
  pp.pprint(final_list)
  
  
def proba_givenb(probb_givena, proba, probb_givennota):
  
  '''#Single Bayes Theorem Test A given B'''
  
  num = probb_givena * proba
  prob_nota = 1 - proba
  denom = num + (probb_givennota*prob_nota)
  return num/denom

def one_samp_pval(conts):
  '''1sample PVALUE from list of continuous value columns
    from a list called conts'''
  for x in conts:
    print(x)
    pvals =(stats.ttest_1samp(pos[x], 0, nan_policy='omit'))
    print(pvals)
    print(pvals[1])
    if float(pvals[1]) > 0.05:
      print("Due to the insignificant p-value we would FAIL TO REJECT..")

def two_samp_pval(conts):
  '''2 TWO SAMPLE test against means of subgroups within a population: beware: means
  comparison is valued differently then taking the mean against a 1samp
  left df in stats test will be the primary, takes list named conts'''
  for x in conts:
    print(x)
    pvals =(stats.ttest_ind(pos[x], neg[x], nan_policy='omit'))
    print(pvals)
    print(pvals[1])
    if float(pvals[1]) > 0.05:
      print("Due to the insignificant p-value we would FAIL TO REJECT..")
  
def zscore(raw_score):
  z = (raw_score - np.mean(raw_score))/stdev
  return z

def create_t_test_set(null_df, reject_df):
    d1 = {}
    d1 = d1.fromkeys(null_df.columns)
    for col in df.columns[1:]:
        stat, p = stats.ttest_ind(null_df[col], reject_df[col])
        d1[col] = [stat,p]
    test_df = pd.DataFrame(d1, index=['t_test','p_value'])
    return test_df 

def confidence_interval(data, confidence=.95):
  n = len(data)
  mean = sum(data)/n
  data = np.array(data)
  stderr = stats.sem(data)
  interval = stderr * stats.t.ppf((1 + confidence) / 2.0, n-1)
  return (mean , mean-interval, mean+interval)



def support_ttest(feature, level, class1, class2):
  
  '''# Function takes a feature from the dataframe and a significance level and 
   returns a tuple with a boolean value (True if the pvalue < significance level) 
   and the pvalue.'''
  
  one = df[df["Class Name"]== class1][feature]
  two = df[df['Class Name']== class2][feature]
  statistic, pvalue = ttest_ind(one, two, nan_policy='omit')
  return pvalue < level, pvalue



def print_ttests(index):
  
  ''' Function takes a column value for the index, prints pvalues and whether or not
  null hypothesis should be rejected for all other features in the dataframe.'''
  
  for column in df.columns:
    if column != index:
      reject, pvalue = support_ttest(column, .05)
      if(reject == True):
        print('The p-value for ' + column +  ' is ' + str(pvalue) + 
              ',\n reject the null hypothesis\n')
      else: 
        print('The p-value for ' + column + ' is ' + str(pvalue) + 
              ',\n accept the null hypothesis\n')
        
#PCA

def get_pca(df,n):
  
  '''Gets the n number of PCA from a dataframe on continuous variables'''

  print("Data Frame")
  print(df.head())
  print()
  dups = df.duplicated()
  print('Number of duplicate rows = %d' % (dups.sum()))
  print()
  print("Variance -1 ddof")
  print()
  print(df.var(axis=0,ddof=1).sort_values())
  print()
  print()
  print("Correlation Matrix")
  print(df.corr())
  print()

  def get_redundant_pairs(df):
      '''Get diagonal and lower triangular pairs of correlation matrix'''
      pairs_to_drop = set()
      cols = df.columns
      for i in range(0, df.shape[1]):
          for j in range(0, i+1):
              pairs_to_drop.add((cols[i], cols[j]))
      return pairs_to_drop

  def get_top_abs_correlations(df, n=5): 
      au_corr = df.corr().abs().unstack()
      labels_to_drop = get_redundant_pairs(df)
      au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)
      return au_corr[0:n]

  print("Top Absolute Correlations")
  print(get_top_abs_correlations(df, n))
  print()
