{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caterpillar Tube Pricing: Predict the prices suppliers will quote for industrial tube assemblies.\n",
    "\n",
    "# Log-Linear Regression, Feature Engineering\n",
    "\n",
    "\n",
    "#### Objectives\n",
    "- log-transform regression target with right-skewed distribution\n",
    "- use regression metric: RMSLE\n",
    "- do feature engineering with relational data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process\n",
    "\n",
    "#### Francois Chollet, [Deep Learning with Python](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/README.md), Chapter 4: Fundamentals of machine learning, \"A universal workflow of machine learning\"\n",
    " \n",
    "> **1. Define the problem at hand and the data on which you’ll train.** Collect this data, or annotate it with labels if need be.\n",
    "\n",
    "> **2. Choose how you’ll measure success on your problem.** Which metrics will you monitor on your validation data?\n",
    "\n",
    "> **3. Determine your evaluation protocol:** hold-out validation? K-fold validation? Which portion of the data should you use for validation?\n",
    "\n",
    "> **4. Develop a first model that does better than a basic baseline:** a model with statistical power.\n",
    "\n",
    "> **5. Develop a model that overfits.** The universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\n",
    "\n",
    "> **6. Regularize your model and tune its hyperparameters, based on performance on the validation data.** Repeatedly modify your model, train it, evaluate on your validation data (not the test data, at this point), modify it again, and repeat, until the model is as good as it can get. \n",
    "\n",
    "> **Iterate on feature engineering: add new features, or remove features that don’t seem to be informative.** Once you’ve developed a satisfactory model configuration, you can train your final production model on all the available data (training and validation) and evaluate it one last time on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the problem 🚜\n",
    "\n",
    "#### [Description](https://www.kaggle.com/c/caterpillar-tube-pricing/overview/description)\n",
    "\n",
    "> Like snowflakes, it's difficult to find two tubes in Caterpillar's diverse catalogue of machinery that are exactly alike. Tubes can vary across a number of dimensions, including base materials, number of bends, bend radius, bolt patterns, and end types.\n",
    "\n",
    "> Currently, Caterpillar relies on a variety of suppliers to manufacture these tube assemblies, each having their own unique pricing model. This competition provides detailed tube, component, and annual volume datasets, and challenges you to predict the price a supplier will quote for a given tube assembly. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the data on which you'll train\n",
    "\n",
    "#### [Data Description](https://www.kaggle.com/c/caterpillar-tube-pricing/data)\n",
    "\n",
    "> The dataset is comprised of a large number of relational tables that describe the physical properties of tube assemblies. You are challenged to combine the characteristics of each tube assembly with supplier pricing dynamics in order to forecast a quote price for each tube. The quote price is labeled as cost in the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data\n",
    "\n",
    "\n",
    "### Option 1. Kaggle web UI\n",
    " \n",
    "Sign in to Kaggle and go to the [Caterpillar Tube Pricing](https://www.kaggle.com/c/caterpillar-tube-pricing) competition. Go to the Data page. After you have accepted the rules of the competition, use the download buttons to download the data.\n",
    "\n",
    "\n",
    "### Option 2. Kaggle API\n",
    "\n",
    "1. [Follow these instructions](https://github.com/Kaggle/kaggle-api#api-credentials) to create a Kaggle “API Token” and download your `kaggle.json` file.\n",
    "\n",
    "2. Put `kaggle.json` in the correct location.\n",
    "\n",
    "  - If you're using Anaconda, put the file in the directory specified in the [instructions](https://github.com/Kaggle/kaggle-api#api-credentials).\n",
    "\n",
    "  - If you're using Google Colab, upload the file to your Google Drive, and run this cell:\n",
    "\n",
    "  ```\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  %env KAGGLE_CONFIG_DIR=/content/drive/My Drive/\n",
    "  ```\n",
    "\n",
    "3. Install the Kaggle API package.\n",
    "```\n",
    "pip install kaggle\n",
    "```\n",
    "\n",
    "4. After you have accepted the rules of the competiton, use the Kaggle API package to get the data.\n",
    "```\n",
    "kaggle competitions download -c caterpillar-tube-pricing\n",
    "```\n",
    "\n",
    "### Option 3. Google Drive\n",
    "\n",
    "Download [zip file](https://drive.google.com/uc?export=download&id=1oGky3xR6133pub7S4zIEFbF4x1I87jvC) from Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip caterpillar-tube-pricing.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get filenames & shapes\n",
    "\n",
    "[Python Standard Library: glob](https://docs.python.org/3/library/glob.html)\n",
    "\n",
    "> The `glob` module finds all the pathnames matching a specified pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import pandas as pd\n",
    "\n",
    "for path in glob('competition_data/*.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    print(path, df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Choose how you'll measure success on your problem\n",
    "\n",
    "> Which metrics will you monitor on your validation data?\n",
    "\n",
    "#### [Evaluation](https://www.kaggle.com/c/caterpillar-tube-pricing/overview/evaluation)\n",
    "\n",
    "> Submissions are evaluated one the Root Mean Squared Logarithmic Error (RMSLE). The RMSLE is calculated as\n",
    ">\n",
    "> $\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\log \\left(p_{i}+1\\right)-\\log \\left(a_{i}+1\\right)\\right)^{2}}$\n",
    ">\n",
    "> Where:\n",
    ">\n",
    "> - $n$ is the number of price quotes in the test set\n",
    "> - $p_i$ is your predicted price\n",
    "> - $a_i$ is the actual price\n",
    "> - $log(x)$ is the natural logarithm\n",
    "\n",
    "#### [Scikit-Learn User Guide](https://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-log-error)\n",
    "\n",
    "> The `mean_squared_log_error` function is best to use when targets have exponential growth, such as population counts, average sales of a commodity over a span of years etc. Note that this metric penalizes an under-predicted estimate greater than an over-predicted estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine your evaluation protocol\n",
    "\n",
    "> Which portion of the data should you use for validation?\n",
    "\n",
    "#### Rachel Thomas, [How (and why) to create a good validation set](https://www.fast.ai/2017/11/13/validation-sets/)\n",
    "\n",
    "> You will want to create your own training and validation sets (by splitting the Kaggle “training” data). You will just use your smaller training set (a subset of Kaggle’s training data) for building your model, and you can evaluate it on your validation set (also a subset of Kaggle’s training data) before you submit to Kaggle.\n",
    "\n",
    "> When is a random subset not good enough?\n",
    "> - Time series\n",
    "> - New people, new boats, new…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the test set have different dates?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Does the test set have different tube assemblies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make the validation set like the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin with baselines for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop a first model that does better than a basic baseline\n",
    "\n",
    "### Fit Random Forest with 1 feature: `quantity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log-transform regression target with right-skewed distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot right-skewed distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Terence Parr & Jeremy Howard, [The Mechanics of Machine Learning, Chapter 5.5](https://mlbook.explained.ai/prep.html#logtarget)\n",
    "\n",
    "> Transforming the target variable (using the mathematical log function) into a tighter, more uniform space makes life easier for any model.\n",
    "\n",
    "> The only problem is that, while easy to execute, understanding why taking the log of the target variable works and how it affects the training/testing process is intellectually challenging. You can skip this section for now, if you like, but just remember that this technique exists and check back here if needed in the future.\n",
    "\n",
    "> Optimally, the distribution of prices would be a narrow “bell curve” distribution without a tail. This would make predictions based upon average prices more accurate. We need a mathematical operation that transforms the widely-distributed target prices into a new space. The “price in dollars space” has a long right tail because of outliers and we want to squeeze that space into a new space that is normally distributed (“bell curved”). More specifically, we need to shrink large values a lot and smaller values a little. That magic operation is called the logarithm or log for short. \n",
    "\n",
    "> To make actual predictions, we have to take the exp of model predictions to get prices in dollars instead of log dollars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wikipedia, [Logarithm](https://en.wikipedia.org/wiki/Logarithm)\n",
    "\n",
    "> Addition, multiplication, and exponentiation are three fundamental arithmetic operations. Addition can be undone by subtraction. Multiplication can be undone by division. The idea and purpose of **logarithms** is also to **undo** a fundamental arithmetic operation, namely raising a number to a certain power, an operation also known as **exponentiation.** \n",
    "\n",
    "> For example, raising 2 to the third power yields 8.\n",
    "\n",
    "> The logarithm (with respect to base 2) of 8 is 3, reflecting the fact that 2 was raised to the third power to get 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Numpy for exponents and logarithms functions\n",
    "- https://docs.scipy.org/doc/numpy/reference/routines.math.html#exponents-and-logarithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Refit model with log-transformed target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interlude: Moore's Law dataset\n",
    "\n",
    "#### Background\n",
    "- https://en.wikipedia.org/wiki/Moore%27s_law\n",
    "- https://en.wikipedia.org/wiki/Transistor_count\n",
    "\n",
    "#### Scrape HTML tables with Pandas!\n",
    "- https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_html.html\n",
    "- https://medium.com/@ageitgey/quick-tip-the-easiest-way-to-grab-data-out-of-a-web-page-in-python-7153cecfca58\n",
    "\n",
    "#### More web scraping options\n",
    "- https://automatetheboringstuff.com/chapter11/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data\n",
    "tables = pd.read_html('https://en.wikipedia.org/wiki/Transistor_count', header=0)\n",
    "moore = tables[0]\n",
    "moore = moore[['Date of introduction', 'Transistor count']].dropna()\n",
    "\n",
    "# Clean data\n",
    "for column in moore:\n",
    "    moore[column] = (moore[column]\n",
    "                     .str.split('[').str[0]  # Remove citations\n",
    "                     .str.replace(r'\\D','')  # Remove non-digit characters\n",
    "                     .astype(int))\n",
    "    \n",
    "moore = moore.sort_values(by='Date of introduction')\n",
    "\n",
    "# Plot distribution of transistor counts\n",
    "sns.distplot(moore['Transistor count']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relationship between date & transistors\n",
    "moore.plot(x='Date of introduction', y='Transistor count', kind='scatter', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log-transform the target\n",
    "moore['log(Transistor count)'] = np.log1p(moore['Transistor count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of log-transformed target\n",
    "sns.distplot(moore['log(Transistor count)']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot relationship between date & log-transformed target\n",
    "moore.plot(x='Date of introduction', y='log(Transistor count)', kind='scatter', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Linear Regression with log-transformed target\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "X = moore[['Date of introduction']]\n",
    "y_log = moore['log(Transistor count)']\n",
    "model.fit(X, y_log)\n",
    "y_pred_log = model.predict(X)\n",
    "\n",
    "# Plot line of best fit, in units of log-transistors\n",
    "ax = moore.plot(x='Date of introduction', y='log(Transistor count)', kind='scatter', alpha=0.5)\n",
    "ax.plot(X, y_pred_log);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert log-transistors to transistors\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "# Plot line of best fit, in units of transistors\n",
    "ax = moore.plot(x='Date of introduction', y='Transistor count', kind='scatter', alpha=0.5)\n",
    "ax.plot(X, y_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to Caterpillar 🚜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select more features\n",
    "\n",
    "#### [Data Description](https://www.kaggle.com/c/caterpillar-tube-pricing/data)\n",
    "\n",
    "> **train_set.csv and test_set.csv**  \n",
    "> This file contains information on price quotes from our suppliers. Prices can be quoted in 2 ways: bracket and non-bracket pricing. Bracket pricing has multiple levels of purchase based on quantity (in other words, the cost is given assuming a purchase of quantity tubes). Non-bracket pricing has a minimum order amount (min_order) for which the price would apply. Each quote is issued with an annual_usage, an estimate of how many tube assemblies will be purchased in a given year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install category_encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do feature engineering with relational data\n",
    "\n",
    "#### [Data Description](https://www.kaggle.com/c/caterpillar-tube-pricing/data)\n",
    "\n",
    "> The dataset is comprised of a large number of relational tables that describe the physical properties of tube assemblies. You are challenged to combine the characteristics of each tube assembly with supplier pricing dynamics in order to forecast a quote price for each tube.\n",
    "\n",
    "> **tube.csv**  \n",
    "> This file contains information on tube assemblies, which are the primary focus of the competition. Tube Assemblies are made of multiple parts. The main piece is the tube which has a specific diameter, wall thickness, length, number of bends and bend radius. Either end of the tube (End A or End X) typically has some form of end connection allowing the tube assembly to attach to other features. Special tooling is typically required for short end straight lengths (end_a_1x, end_a_2x refer to if the end length is less than 1 times or 2 times the tube diameter, respectively). Other components can be permanently attached to a tube such as bosses, brackets or other custom features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in glob('competition_data/*.csv'):\n",
    "    df = pd.read_csv(path)\n",
    "    shared_columns = set(df.columns) & set(train.columns)\n",
    "    if shared_columns:\n",
    "        print(path, df.shape)\n",
    "        print(df.columns.tolist(), '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## see Caterpillar.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
